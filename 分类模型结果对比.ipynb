{"cells":[{"metadata":{"_cell_guid":"ceee4bc5-5a14-c005-2c2a-7c879c3fa0ec","trusted":true},"cell_type":"code","source":"import sklearn\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas\nfrom sklearn.model_selection import train_test_split\nimport numpy","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"17a6b9e2-26da-b492-2107-73611e20d9c4","trusted":true},"cell_type":"code","source":"Tweet= pandas.read_csv(\"../input/twitter-airline-sentiment/Tweets.csv\")\nTweet.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"405b2e80-e9a1-4a93-f0ba-9cbe3bea13b1","trusted":true},"cell_type":"code","source":"# (len(Tweet)-Tweet.count())/len(Tweet)\n\n# Mood_count=Tweet['airline_sentiment'].value_counts()\n\n# Index = [1,2,3]\n# plt.bar(Index,Mood_count)\n# plt.xticks(Index,['negative','neutral','positive'],rotation=45)\n# plt.ylabel('Mood Count')\n# plt.xlabel('Mood')\n# plt.title('Count of Moods')\n\n# Tweet['airline'].value_counts()\n\n# def plot_sub_sentiment(Airline):\n#     df=Tweet[Tweet['airline']==Airline]\n#     count=df['airline_sentiment'].value_counts()\n#     Index = [1,2,3]\n#     plt.bar(Index,count)\n#     plt.xticks(Index,['negative','neutral','positive'])\n#     plt.ylabel('Mood Count')\n#     plt.xlabel('Mood')\n#     plt.title('Count of Moods of '+Airline)\n# plt.figure(1,figsize=(12, 12))\n# plt.subplot(231)\n# plot_sub_sentiment('US Airways')\n# plt.subplot(232)\n# plot_sub_sentiment('United')\n# plt.subplot(233)\n# plot_sub_sentiment('American')\n# plt.subplot(234)\n# plot_sub_sentiment('Southwest')\n# plt.subplot(235)\n# plot_sub_sentiment('Delta')\n# plt.subplot(236)\n# plot_sub_sentiment('Virgin America')\n\n# From the above plots one can find that the distribution of moods for the first three airlines are always skewed toward negative moods. On contrary, the moods are distributed more balanced with the later three airline companies. \n\n# NR_Count=dict(Tweet['negativereason'].value_counts(sort=False))\n\n# def NR_Count(Airline):\n#     if Airline=='All':\n#         df=Tweet\n#     else:\n#         df=Tweet[Tweet['airline']==Airline]\n#     count=dict(df['negativereason'].value_counts())\n#     Unique_reason=list(Tweet['negativereason'].unique())\n#     Unique_reason=[x for x in Unique_reason if str(x) != 'nan']\n#     Reason_frame=pandas.DataFrame({'Reasons':Unique_reason})\n#     Reason_frame['count']=Reason_frame['Reasons'].apply(lambda x: count[x])\n#     return Reason_frame\n\n# def plot_reason(Airline):\n#     df=NR_Count(Airline)\n#     count=df['count']\n#     Index = range(1,(len(df)+1))\n#     plt.bar(Index,count)\n#     plt.xticks(Index,df['Reasons'],rotation=90)\n#     plt.ylabel('Count')\n#     plt.xlabel('Reason')\n#     plt.title('Count of Reasons for '+Airline)\n\n# plot_reason('All')\n\n# plot_reason('US Airways')\n\n# plot_reason('United')\n\n# plot_reason('American')\n\n# plot_reason('Southwest')\n\n# plot_reason('Delta')\n\n# plot_reason('Virgin America')\n\n\n\n# ### D: Word Cloud for the negative Tweets\n\n# from wordcloud import WordCloud,STOPWORDS\n\n# df=Tweet[Tweet['airline_sentiment']=='negative']\n# words = ' '.join(df['text'])\n# cleaned_word = \" \".join([word for word in words.split()\n#                             if 'http' not in word\n#                                 and not word.startswith('@')\n#                                 and word != 'RT'\n#                             ])\n\n# wordcloud = WordCloud(stopwords=STOPWORDS,\n#                       background_color='black',\n#                       width=3000,\n#                       height=2500\n#                      ).generate(cleaned_word)\n\n# plt.figure(1,figsize=(12, 12))\n# plt.imshow(wordcloud)\n# plt.axis('off')\n# plt.show()\n\n# **We can find that the Tweets with negative moods are frequently involved some words like cancelled, flight ,customer or hour. People might guess that customer tends to complain when they are waiting for the delayed flights.**\n\n# ### E: Preprocess data for classification\n\n# **Our data exploration ends up at here. The next step will be preprocess the data in order to make the learning process more smooth.**\n\n\n\n#\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7efe1456-8808-112b-4c7b-20d2259186cf"},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"_cell_guid":"964886df-3b7b-4426-48bc-5e3995c72042","trusted":true},"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\n# 数据清洗\n\ndef tweet_to_words(raw_tweet):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",raw_tweet) \n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return( \" \".join( meaningful_words )) \n\ndef clean_tweet_length(raw_tweet):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",raw_tweet) \n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return(len(meaningful_words)) \n# 将标签转换成数字\nTweet['sentiment']=Tweet['airline_sentiment'].apply(lambda x: 0 if x=='negative' else 1)\nTweet['clean_tweet']=Tweet['text'].apply(lambda x: tweet_to_words(x))\nTweet['Tweet_length']=Tweet['text'].apply(lambda x: clean_tweet_length(x))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d3321d7e-3c68-a64b-8312-aebab5e8ba00","trusted":true},"cell_type":"code","source":"\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train,test = train_test_split(Tweet,test_size=0.2,random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"71718f91-ec73-de31-7dbe-f07444d435f1"},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"_cell_guid":"4813043f-73fb-3474-f7f6-18b45f3543fd","trusted":true},"cell_type":"code","source":"# 转换成list，方便特征提取\ntrain_clean_tweet=[]\nfor tweet in train['clean_tweet']:\n    train_clean_tweet.append(tweet)\ntest_clean_tweet=[]\nfor tweet in test['clean_tweet']:\n    test_clean_tweet.append(tweet)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[](http://)特征提取：参考文档：https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"_cell_guid":"3e2be258-a9a9-9415-3d51-58f32413247e","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nv = CountVectorizer(analyzer = \"word\")\ntrain_features= v.fit_transform(train_clean_tweet)\ntest_features=v.transform(test_clean_tweet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5752b349-7f4f-aa5e-9a07-c577e475d3a4","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9c491112-34ec-e23f-8301-bf5491ef7416","trusted":true},"cell_type":"code","source":"Classifiers = [\n    LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=200),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    XGBClassifier(),\n    MLPClassifier(solver='sgd', alpha=1e-5,\n       hidden_layer_sizes=(5, 2), random_state=1, max_iter=500),\n    GradientBoostingClassifier(random_state=0)\n    \n]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3537c590-cc9c-48bf-8a4e-7f0406a564ff","trusted":true},"cell_type":"code","source":"dense_features=train_features.toarray()\ndense_test= test_features.toarray()\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(train_features,train['sentiment'])\n        pred = fit.predict(test_features)\n    except Exception:\n        fit = classifier.fit(dense_features,train['sentiment'])\n        pred = fit.predict(dense_test)\n    predictions = [round(value) for value in pred]\n    accuracy = accuracy_score(test['sentiment'],predictions)\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+ ' is: '+str(accuracy))    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"131c9c37-3fab-dc5b-3615-ff06ccadafec"},"cell_type":"markdown","source":"## Compare the model performances","execution_count":null},{"metadata":{"_cell_guid":"59679456-286a-cfb3-a8d0-1895cd4af069","trusted":true},"cell_type":"code","source":"Index = [1,2,3,4,5,6,7,8,9]\nplt.bar(Index,Accuracy,alpha=0.9, width = 0.6, \n        facecolor = 'green', edgecolor = 'white' )\nplt.xticks(Index, Model,rotation=90)\nplt.ylabel('Accuracy')\nplt.xlabel('Model')\nplt.title('Accuracies of Models')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2dfa7c03-4d88-4674-148f-2dcc6ad89b3e"},"cell_type":"markdown","source":"","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}